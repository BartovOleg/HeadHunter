{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Бартов Олег Борисович\n",
    "## Решение задачи по предсказанию набора специализаций для вакансии\n",
    "### 2020-07-27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#стандартные библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#для загрузки данных\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "#для очистки htmp-тегов\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#прогресс выполнения\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#для векторайзеры\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#пакетная кластеризация KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "#для мультилейбл\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#бустинг\n",
    "import catboost\n",
    "\n",
    "#процедура для загрузки вакансий в словарь\n",
    "def read_vacancies_part(part):\n",
    "    with gzip.open(f'vacancies-{part:02}.json.gz', 'r') as fp:\n",
    "        return json.loads(fp.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# №1 Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#объявляем стемминг\n",
    "stemmer = SnowballStemmer('russian')\n",
    "#запускаем цикл по частям\n",
    "for part_num in range(1, 11):\n",
    "    \n",
    "    #грузим часть\n",
    "    part = read_vacancies_part(part_num)\n",
    "    print('Часть '+str(part_num))\n",
    "    print(f'Всего вакансий в части: {len(part):,d}')\n",
    "          \n",
    "    #запускаем цикл для препроцессинга текста\n",
    "    for k in tqdm(part):\n",
    "          \n",
    "        #работаем с наименованием вакансии\n",
    "        transf_name = part[k]['name']\n",
    "        #приводим все к нижнему регистру\n",
    "        transf_name = transf_name.lower()\n",
    "        #убираем знаки препинания и цифры\n",
    "        transf_name = re.sub(r\"[^a-zа-я]+\",' ', transf_name)\n",
    "        #убираем лишние пробелы\n",
    "        transf_name = transf_name.strip()\n",
    "        #делаем стемминг - получается массив\n",
    "        transf_name = [stemmer.stem(word) for word in transf_name.split(' ')]\n",
    "        #возвращаем строку из массива обратно\n",
    "        transf_name = ' '.join(map(str, transf_name))\n",
    "        #переписываем обработанное наименование вакансии\n",
    "        part[k]['name'] = transf_name\n",
    "\n",
    "        \n",
    "        #теперь работаем с описанием\n",
    "        #убираем html-теги\n",
    "        transf_desc = BeautifulSoup(part[k]['description'], \"lxml\").text\n",
    "        #приводим все к нижнему регистру\n",
    "        transf_desc = transf_desc.lower()\n",
    "        #убираем знаки препинания и цифры\n",
    "        transf_desc = re.sub(r\"[^a-zа-я]+\",' ', transf_desc)\n",
    "        #убираем лишние пробелы\n",
    "        transf_desc = transf_desc.strip()\n",
    "        #делаем стемминг - получается массив\n",
    "        transf_desc = [stemmer.stem(word) for word in transf_desc.split(' ')]        \n",
    "        #возвращаем строку из массива обратно\n",
    "        transf_desc = ' '.join(map(str, transf_desc))\n",
    "        #переписываем обработанное описание вакансии\n",
    "        part[k]['description'] = transf_desc\n",
    "        \n",
    "        \n",
    "        #работаем с ключевыми навыками вакансии\n",
    "        transf_skills = part[k]['key_skills']\n",
    "        #из массива в строку\n",
    "        transf_skills = ' '.join(map(str, transf_skills))\n",
    "        #приводим все к нижнему регистру\n",
    "        transf_skills = transf_skills.lower()\n",
    "        #убираем лишние пробелы\n",
    "        transf_skills = transf_skills.strip()\n",
    "        #делаем стемминг - получается массив\n",
    "        transf_skills = [stemmer.stem(word) for word in transf_skills.split(' ')]\n",
    "        #возвращаем строку из массива обратно\n",
    "        transf_skills = ' '.join(map(str, transf_skills))\n",
    "        part[k]['key_skills'] = transf_skills\n",
    "        \n",
    "        #решаем вопрос с зарплатой - это должен стать один признак\n",
    "        salary = part[k]['compensation_to']\n",
    "        if salary is None:\n",
    "            salary = part[k]['compensation_from']\n",
    "        if salary is None:\n",
    "            salary = 0\n",
    "        part[k]['salary'] = salary\n",
    "        #убираем лишнее\n",
    "        part[k].pop('compensation_to')\n",
    "        part[k].pop('compensation_from')\n",
    "    \n",
    "    #записываем часть, которую преобразовали в файл, дальше будем эти файлы объединять\n",
    "    pd.DataFrame(part).T.to_csv('df_stem'+str(part_num)+'.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### объединяем обработанные части в единый датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv('df_stem1.csv',sep=';') #df_part1 #df_clean_morhp1\n",
    "print(1)\n",
    "df = df.append(pd.read_csv('df_stem2.csv',sep=';')) #df_part2 #df_clean_morhp2\n",
    "print(2)\n",
    "df = df.append(pd.read_csv('df_stem3.csv',sep=';')) #df_part3 #df_clean_morhp3\n",
    "print(3)\n",
    "df = df.append(pd.read_csv('df_stem4.csv',sep=';')) #df_part4 #df_clean_morhp4\n",
    "print(4)\n",
    "df = df.append(pd.read_csv('df_stem5.csv',sep=';')) #df_part5 #df_clean_morhp5\n",
    "print(5)\n",
    "df = df.append(pd.read_csv('df_stem6.csv',sep=';')) #df_part6 #df_clean_morhp6\n",
    "print(6)\n",
    "df = df.append(pd.read_csv('df_stem7.csv',sep=';')) #df_part7 #df_clean_morhp7\n",
    "print(7)\n",
    "df = df.append(pd.read_csv('df_stem8.csv',sep=';')) #df_part8 #df_clean_morhp8\n",
    "print(8)\n",
    "df = df.append(pd.read_csv('df_stem9.csv',sep=';')) #df_part9 #df_clean_morhp9\n",
    "print(9)\n",
    "df = df.append(pd.read_csv('df_stem10.csv',sep=';')) #df_part10 #df_clean_morhp10\n",
    "print(10)\n",
    "\n",
    "df.rename(columns={'Unnamed: 0': 'vac_id'}, inplace=True) #важная колонка - ключ\n",
    "df.reset_index(inplace=True) #так как объединение, имеет смысл сбросить индекс\n",
    "del df['index'] #лишний столбец\n",
    "\n",
    "#сразу формируем категориальные признаки\n",
    "\n",
    "#зп\n",
    "df['sal_clust'] = (df['salary']/1000).astype('int64').astype('category').cat.codes\n",
    "#валюта\n",
    "df['currency'].fillna('RUR',inplace=True) #если не указано, пусть будет рубль\n",
    "df['currency'] = df['currency'].astype('category').cat.codes\n",
    "#остальные просто преобразуем в категории\n",
    "df['employer'] = df['employer'].astype('category').cat.codes\n",
    "df['employment'] = df['employment'].astype('category').cat.codes\n",
    "df['work_schedule'] = df['work_schedule'].astype('category').cat.codes\n",
    "df['work_experience'] = df['work_experience'].astype('category').cat.codes\n",
    "\n",
    "#записываем объединенный датасет файл\n",
    "df.to_csv('df_stem.csv',sep=';',index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# №2 Кластеризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#читаем сохраненный ранее файл (лучше предобработку запускать отдельно из-за проблем с памятью, когда ее мало)\n",
    "df = pd.read_csv('df_stem.csv',sep=';', parse_dates=['creation_date'])\n",
    "\n",
    "#дата подачи вакансии еще один категориальный признак - пытаемся поймать сезонность\n",
    "df['date_feat'] = df['creation_date'].dt.strftime('%Y-%m')\n",
    "df['date_feat'] = df['date_feat'].astype('category').cat.codes\n",
    "del df['creation_date']\n",
    "\n",
    "#смотрим, что прочитали именно то, что нужно\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#выполняем преобразование, иначе в векторайзер не залезает\n",
    "df['name'] = df['name'].astype('str')\n",
    "df['description'] = df['description'].astype('str')\n",
    "df['key_skills'] = df['key_skills'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### № 2.1 Формируем кластеры по наименованию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на мешок слов наименования\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "matrix = vectorizer.fit_transform(df['name'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=1800, max_iter=10, init_size=5400, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_name_1800',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на биграммы наименования\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(2,2))\n",
    "matrix = vectorizer.fit_transform(df['name'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=1500, max_iter=10, init_size=4500, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_2gram_name_1500',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на триграммы наименования\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(3,3))\n",
    "matrix = vectorizer.fit_transform(df['name'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=2000, max_iter=10, init_size=6000, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_3gram_name_2000',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#count на мешок слов наименования\n",
    "vectorizer = CountVectorizer(min_df=2)\n",
    "matrix = vectorizer.fit_transform(df['name'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=1500, max_iter=10, init_size=4500, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_count_name_1500',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#count на биграммы наименования\n",
    "vectorizer = CountVectorizer(min_df=2, ngram_range=(2,2))\n",
    "matrix = vectorizer.fit_transform(df['name'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=1500, max_iter=10, init_size=4500, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_count_2gram_name_1500',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### №2.2 Формируем кластеры по описанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на мешок слов описания\n",
    "vectorizer = TfidfVectorizer(min_df=10, max_df=100000)\n",
    "matrix = vectorizer.fit_transform(df['description'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=2000, max_iter=10, init_size=6000, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_description_2000',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на мешок слов другой частотности описания\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df=1000)\n",
    "matrix = vectorizer.fit_transform(df['description'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=5000, max_iter=10, init_size=15000, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_description_s_2000',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на биграммы описания\n",
    "vectorizer = TfidfVectorizer(max_df=50000,min_df=200, ngram_range=(2,2))\n",
    "matrix = vectorizer.fit_transform(df['description'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=2000, max_iter=10, init_size=6000, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_2gramm_description_2000',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на триграммы описания\n",
    "vectorizer = TfidfVectorizer(max_df=50000,min_df=150, ngram_range=(2,2))\n",
    "matrix = vectorizer.fit_transform(df['description'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=2000, max_iter=10, init_size=6000, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_2gramm_description_2000',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на мешок слов и биграммы описания совместно\n",
    "vectorizer = TfidfVectorizer(max_df=50000,min_df=200, ngram_range=(1,2))\n",
    "matrix = vectorizer.fit_transform(df['description'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=2000, max_iter=10, init_size=6000, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_both_description_2000',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### №2.3 Формируем кластеры по ключевым навыкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на мешок слов ключевых навыков\n",
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "matrix = vectorizer.fit_transform(df['key_skills'])\n",
    "print(matrix.shape)\n",
    "\n",
    "#кластеризация\n",
    "minikmeans = MiniBatchKMeans(n_clusters=2000, max_iter=10, init_size=6000, batch_size=100).fit(matrix)\n",
    "#сохраняем результат кластеризации во внешний файл\n",
    "np.save('clust_skills_2000',minikmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на биграммы наименования\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(2,2))\n",
    "matrix = vectorizer.fit_transform(df['name'])\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tf-idf на триграммы наименования\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(3,3))\n",
    "matrix = vectorizer.fit_transform(df['name'])\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf на мешок слов описания\n",
    "vectorizer = TfidfVectorizer(min_df=10, max_df=100000)\n",
    "matrix = vectorizer.fit_transform(df['description'])\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf на мешок слов другой частотности описания\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df=1000)\n",
    "matrix = vectorizer.fit_transform(df['description'])\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf на биграммы описания\n",
    "vectorizer = TfidfVectorizer(max_df=50000,min_df=200, ngram_range=(2,2))\n",
    "matrix = vectorizer.fit_transform(df['description'])\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf на триграммы описания\n",
    "vectorizer = TfidfVectorizer(max_df=50000,min_df=150, ngram_range=(2,2))\n",
    "matrix = vectorizer.fit_transform(df['description'])\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf на мешок слов и биграммы описания совместно\n",
    "vectorizer = TfidfVectorizer(max_df=50000,min_df=200, ngram_range=(1,2))\n",
    "matrix = vectorizer.fit_transform(df['description'])\n",
    "print(matrix.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
